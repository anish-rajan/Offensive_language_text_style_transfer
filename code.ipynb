{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n!pip install torchviz\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.utils.data as data\nimport torchvision.models as models\nimport torchvision.datasets as dset\nimport torchvision.transforms as transforms\nfrom torch.autograd import Variable\nfrom torchvision.models.vgg import model_urls\nfrom torchviz import make_dot\nimport torch.nn.functional as F\nimport random\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n!pip install GPUtil\n\nimport torch\nfrom GPUtil import showUtilization as gpu_usage\nfrom numba import cuda \n\n\nrandom_state=2019","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv('../input/abusive-language/hate_dataset.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torchtext.data.utils import get_tokenizer\nfrom collections import Counter\nfrom torchtext.vocab import Vocab\n\ntokenizer = get_tokenizer('basic_english')\ncounter = Counter()\nfor i in data.tweet:\n    counter.update(tokenizer(i))\n# start_and_end = '<start> <end>'\n# counter.update(tokenizer(start_and_end))\nvocab = Vocab(counter, min_freq=1,specials = ['<unk>','<pad>','<start>','<end>'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_combined(data):\n    combined_data = []\n    for i in range(len(data)):\n        combined_data.append((data.iloc[i].tweet,data.iloc[i].target))\n    return combined_data\n\ncombined_data = generate_combined(data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text_pipeline = lambda x: [vocab[token] for token in tokenizer(x)]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain_data,validation_data = train_test_split(combined_data,shuffle=True,random_state=random_state,stratify=data.target,test_size=0.2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\ndef collate_batch(batch):\n    target, offsets = [], []\n    pad_token = vocab['<pad>']\n    lens = [len(tokenizer(text)) for (text,target) in batch]\n    batch_size = len(lens)\n    longest_sent = max(lens) + 2\n    padded_text = np.ones((batch_size, longest_sent)) * pad_token\n    for i,(_text, _target) in enumerate(batch):\n         target.append(_target)\n         temp_text = '<start> ' + _text + ' <end>'  \n         sentence_length = len(tokenizer(temp_text))\n         processed_text = torch.tensor(text_pipeline(temp_text), dtype=torch.int64)\n         padded_text[i,:sentence_length] = processed_text\n         offsets.append([processed_text.size(0)])\n    target = torch.tensor([t for t in target],dtype=torch.long)\n    target = target.unsqueeze(1)\n    offsets = torch.tensor(offsets)\n    padded_text = torch.tensor(padded_text,dtype=torch.int64)\n    return padded_text.to(device), target.to(device), offsets.to(device)\n\ntrain_dataloader = DataLoader(train_data, batch_size=64, shuffle=True, collate_fn=collate_batch)\nvalidation_dataloader = DataLoader(validation_data, batch_size=64, shuffle=True, collate_fn=collate_batch)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Encoder(nn.Module):\n    def __init__(self, embedding_dim, encoder_hidden_dim,embedding_layer,n_layers=1, dropout_prob=0.5):\n        super().__init__()\n \n        self.embedding = embedding_layer\n        self.encoderLabelsTransform = torch.nn.Linear(1, encoder_hidden_dim)\n        self.encoder = nn.GRU(embedding_dim, encoder_hidden_dim, n_layers,batch_first=True)\n        self.dropout = nn.Dropout(dropout_prob)\n \n    def forward(self, input_batch,label):\n        embedded = self.dropout(self.embedding(input_batch))\n        tensorlabel = self.encoderLabelsTransform(label)\n        tensorlabel = tensorlabel.unsqueeze(0)\n        outputs, hidden = self.encoder(embedded,tensorlabel)\n        outputs = self.dropout(outputs)\n \n        return outputs, hidden","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BahdanauDecoder(nn.Module):\n      def __init__(self, embedding_dim,hidden_size, output_size, embedding_layer, n_layers=1, drop_prob=0.1):\n        super(BahdanauDecoder, self).__init__()\n        self.hidden_size = hidden_size\n        self.output_size = output_size\n        self.n_layers = n_layers\n        self.drop_prob = drop_prob\n\n        self.embedding = embedding_layer\n\n        self.fc_hidden = nn.Linear(self.hidden_size, self.hidden_size, bias=False)\n        self.fc_encoder = nn.Linear(self.hidden_size, self.hidden_size, bias=False)\n        self.weight = nn.Parameter(torch.randn(hidden_size,1))\n        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n        self.dropout = nn.Dropout(self.drop_prob)\n        self.gru = nn.GRU(self.hidden_size+embedding_dim, self.hidden_size, batch_first=True)\n        self.classifier = nn.Linear(self.hidden_size, self.output_size)\n\n      def forward(self, inputs, hidden, encoder_outputs):\n#         encoder_outputs = encoder_outputs.squeeze()\n        # Embed input words\n        embedded = self.embedding(inputs)\n        embedded = embedded.view(encoder_outputs.shape[0],-1)\n        embedded = self.dropout(embedded)\n\n        # Calculating Alignment Scores\n        x = torch.tanh(self.fc_hidden(torch.transpose(hidden,0,1))+self.fc_encoder(encoder_outputs))\n        weight = self.weight.repeat(encoder_outputs.shape[0],1,1)\n        alignment_scores = x.bmm(weight)  \n\n        # Softmaxing alignment scores to get Attention weights\n        attn_weights = F.softmax(alignment_scores, dim=1)\n\n        # Multiplying the Attention weights with encoder outputs to get the context vector\n        context_vector = torch.bmm(torch.transpose(attn_weights,1,2),\n                                 encoder_outputs)\n        context_vector = torch.transpose(context_vector,0,1)\n        # Concatenating context vector with embedded input word\n        output = torch.cat((embedded, context_vector[0]), 1).unsqueeze(1)\n        # Passing the concatenated vector as input to the LSTM cell\n        output, hidden = self.gru(output, hidden)\n        # Passing the LSTM output through a Linear layer acting as a classifier\n        output = F.softmax(self.classifier(output).view(encoder_outputs.shape[0],-1), dim=1)\n        return output, hidden, attn_weights","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Decoder(nn.Module):\n    def __init__(self, one_step_decoder,device):\n        super().__init__()\n        self.one_step_decoder = one_step_decoder\n        self.decoderLabelsTransform = nn.Linear(1,self.one_step_decoder.hidden_size)\n        self.device = device\n \n    def forward(self,encoder_outputs, label, start_token,target=None,lengths=None,teacher_forcing_ratio=1,max_length=20):\n        sort_ind = None\n        if target is not None:\n            lengths, sort_ind = lengths.squeeze(1).sort(dim=0, descending=True)\n            encoder_outputs = encoder_outputs[sort_ind]\n            target = target[sort_ind]\n            label = label[sort_ind]\n        else:\n            teacher_forcing_ratio = 0\n        batch_size = encoder_outputs.shape[0]\n        trg_len = target.shape[1]-1 if target is not None else max_length\n        trg_vocab_size = self.one_step_decoder.output_size\n        hidden = self.decoderLabelsTransform(label)\n        hidden = hidden.unsqueeze(0)\n        \n        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device) \n        top1 = (torch.ones(batch_size,dtype=torch.long)*start_token).to(self.device)\n        for t in range(trg_len):\n            # Pass the encoder_outputs. For the first time step the \n            # hidden state comes from the encoder model.\n            batch_size_t = sum([l > t for l in lengths]) if lengths is not None else batch_size\n            teacher_force = random.random() < teacher_forcing_ratio\n            inputs = target[:batch_size_t,t].unsqueeze(1) if teacher_force else top1[:batch_size_t].unsqueeze(1)\n            output, hidden, a = self.one_step_decoder(inputs, hidden[:,:batch_size_t], encoder_outputs[:batch_size_t])\n            outputs[:batch_size_t,t] = output\n\n            top1 = output.argmax(1)\n            \n        return outputs,target,label,sort_ind","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BeamState(object):\n\n    def __init__(self, word, h, outputs,sentence, nll):\n        \"\"\"\n        Args:\n            word -- the id of the word characterising the state\n            h -- the hidden state associated to that state\n            sentence -- a list of word ids (the past ids plus the current one)\n            nll -- the negative log likelihood corresponding to the sentence\n        \"\"\"\n        self.word, self.h, self.outputs, self.sentence, self.nll = word, h, outputs, sentence, nll\n\n\nclass BeamSearchDecoder(object):\n\n    def __init__(self, styleTransfer):\n        self.model = styleTransfer\n        self.max_length = self.model.max_len\n        self.width = self.model.beam_width\n\n    def _decode(self, tokens, encoder_outputs, h):\n        \"\"\"\n        Args:\n            tokens --\n            h --\n        Outputs:\n            logProbs --\n            indices --\n            h --\n        \"\"\"\n        currTokens = tokens.unsqueeze(1)\n        currh = h\n        # generate next h state and logit\n        # generator needs input (seq_len, batch_size, input_size)\n        vocabProbs, h, _ = self.model.decoder.one_step_decoder(currTokens, currh, encoder_outputs)\n        # beam search trick to prevent probs vanishing\n        logProbs = torch.log(vocabProbs)\n        # take the beam_with most probable words\n        logProbs, indices = torch.topk(logProbs, self.width, dim=-1)\n        return vocabProbs,logProbs, indices, h\n\n    def _beamDecode(self, encoder_outputs,h0):\n        \"\"\"\n        Returning the ids of the beam_width most probable sentences' words.\n        Args:\n            h0 -- the first hidden state of dim = dim_y + dim_z\n        \"\"\"\n        batch_size = h0.shape[1]\n        go = torch.tensor([self.model.vocab['<start>']] * batch_size,dtype=torch.long).to(self.model.device)\n        init_state = BeamState(\n            go,\n            h0,\n            [[] for _ in range(batch_size)],\n            [[self.model.vocab['<start>']] for i in range(batch_size)],\n            [0]*batch_size)\n        beam = [init_state]\n        for _ in range(self.max_length):\n            storeBeamLayer = [[] for _ in range(batch_size)]\n            for state in beam:\n                vocabProbs, logProbs, indices, h = self._decode(state.word,encoder_outputs, state.h)\n                for b in range(batch_size):\n                    for w in range(self.width):\n                        word = int(indices[b, w])\n                        storeBeamLayer[b].append(\n                            BeamState(word,\n                                      h[:, b, :],\n                                      state.outputs[b] + [vocabProbs[b]],\n                                      state.sentence[b] + [indices[b, w]],\n                                      state.nll[b] - logProbs[b, w]))\n\n            beam = [init_state for _ in range(self.width)]\n            for b in range(batch_size):\n                # sort beam states by their probability (cumulated nll)\n                # TODO check if performance increase by dividing nll\n                # by number of words\n                sortedBeamLayer = sorted(storeBeamLayer[b], key=lambda k: k.nll)\n                for w in range(self.width):\n                    beam[w].word[b] = sortedBeamLayer[w].word\n                    beam[w].h[:, b, :] = sortedBeamLayer[w].h\n                    beam[w].outputs[b] = sortedBeamLayer[w].outputs\n                    beam[w].sentence[b] = sortedBeamLayer[w].sentence\n                    beam[w].nll[b] = sortedBeamLayer[w].nll\n\n        # Returning the ids of the beam_width most probable sentences' words.\n        sentences = torch.tensor(beam[0].sentence,dtype=torch.long)\n        word_sentences = [\n            [self.model.vocab.itos[i] for i in sent]\n            for sent in sentences]\n        # TODO strip the EOS\n        word_sentences = list(map(lambda x: \" \".join(x), word_sentences))\n        \n        #Outputs of decoder at every time step\n        outputs = beam[0].outputs\n        for i in range(len(outputs)):\n            outputs[i] = torch.tensor([t.to(torch.device('cpu')).detach().numpy() for t in outputs[i]],dtype = torch.float32)\n        outputs = torch.tensor([t.numpy() for t in outputs],dtype=torch.float32)\n        return outputs.to(device),sentences,word_sentences\n\n    def rewriteBatch(self, encoder_outputs,labels,word_id=True):\n        h0 = self.model.decoder.decoderLabelsTransform(labels)\n        h0 = h0.unsqueeze(0)\n        outputs = self._beamDecode(encoder_outputs,h0)\n        if word_id:\n            return outputs[0],torch.tensor(outputs[1],dtype=torch.long).to(device)\n        else:\n            return outputs[2],None","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.nn.utils.rnn import pack_padded_sequence\nclass Model(nn.Module):\n\n    def __init__(self, encoder, decoder,embedding_layer,embedding_dim,Ks,C,beam_width,max_len,vocab,device):\n        super(Model, self).__init__()\n        self.beam_width = beam_width\n        self.max_len = max_len\n        self.device = device\n        self.vocab = vocab\n        self.encoder = encoder\n        self.decoder = decoder\n        self.embedding = embedding_layer\n        self.convs = nn.ModuleList([nn.Conv2d(1, 128, (K,len(vocab)),stride=1) for K in Ks])\n        '''\n        self.conv13 = nn.Conv2d(Ci, Co, (3, D))\n        self.conv14 = nn.Conv2d(Ci, Co, (4, D))\n        self.conv15 = nn.Conv2d(Ci, Co, (5, D))\n        '''\n        self.dropout = nn.Dropout(0.5)\n        self.fc1 = nn.Linear(len(Ks)*128, C)\n        \n        self.optimizer = optim.Adam([{'params':self.encoder.encoderLabelsTransform.parameters()},\n                                     {'params':self.encoder.encoder.parameters()},\n                                    {'params':self.decoder.parameters()},\n                                    {'params':self.convs.parameters(),'lr':0.0005},\n                                    {'params':self.fc1.parameters(),'lr':0.0005}])\n        self.criterion = nn.CrossEntropyLoss().to(device)\n                \n          \n    \n    def _zero_grad(self):\n        self.optimizer.zero_grad()\n    \n    def encoder_decoder(self,inputs,targets,labels1,labels2,lens):\n        encoder_outputs,_ = encoder(inputs,labels1)\n        return decoder(encoder_outputs,labels2,self.vocab['<start>'],targets,lens) \n    \n    def encoder_beam_decoder(self,inputs,targets,labels1,labels2,lens):\n        encoder_outputs,_ = encoder(inputs,labels1)\n        beam = BeamSearchDecoder(self)\n        return beam.rewriteBatch(encoder_outputs,labels2)\n    \n    \n    def classifier(self,generated_inputs):\n        x = [F.leaky_relu(conv(generated_inputs), negative_slope=0.01).squeeze(3)\n            for conv in self.convs]\n        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]\n\n        x = torch.cat(x, 1)\n        x = self.dropout(x)\n        x = self.fc1(x)\n        return x\n    \n    def forward(self,inputs,labels,lens):        \n        batch_size, seq_len = inputs.size()\n        \n        autoencoder_outputs, autoencoder_classifier_outputs,style_converted_classifier_outputs, reconverted_outputs,reconverted_classifier_outputs,sort_ind = None,None,None,None,None,None\n        \n#         Autoencoder\n        autoencoder_outputs,inputs,labels,sort_ind = self.encoder_decoder(inputs,inputs,labels,labels,lens)\n        autoencoder_outputs = self.dropout(autoencoder_outputs)\n        lens = lens[sort_ind]\n\n        \n#         Classifier 1\n#         new_input = torch.zeros(batch_size,seq_len,len(vocab)).to(self.device)\n#         for i in range(batch_size):\n#             for j in range(seq_len):\n#                 new_input[i][j][inputs[i][j]] = 1\n#         new_input = new_input[:,1:]\n        autoencoder_classifier_outputs = self.classifier(autoencoder_outputs.unsqueeze(1))\n        #Backprop\n        trgs = inputs[:,1:]\n        trgs = pack_padded_sequence(trgs, (lens-1).squeeze(1).to(torch.device('cpu')), batch_first=True)\n        autoencoder_scores = pack_padded_sequence(autoencoder_outputs, (lens-1).squeeze(1).to(torch.device('cpu')), batch_first=True)\n        loss1 = self.criterion(autoencoder_scores.data,trgs.data)\n        loss1.backward(retain_graph=True)\n        loss2 = self.criterion(autoencoder_classifier_outputs,torch.tensor(labels,dtype=torch.long).squeeze(1))\n        loss2.backward(retain_graph=True)\n        clip_gradient(self.optimizer,grad_clip)\n        self.optimizer.step()\n        self._zero_grad()\n        \n        \n        #Style Conversion to other style\n        style_outputs,next_style_inputs = self.encoder_beam_decoder(inputs,None,labels,1-labels,None)\n        style_outputs = self.dropout(style_outputs)\n        next_style_inputs = next_style_inputs[:,1:]\n        \n        #Classifier 2\n        style_converted_classifier_outputs = self.classifier(style_outputs.unsqueeze(1))\n        \n        #Backprop\n        loss3 = self.criterion(style_converted_classifier_outputs,1-torch.tensor(labels,dtype=torch.long).squeeze(1))\n        loss3.backward(retain_graph=True)\n        clip_gradient(self.optimizer,grad_clip)\n        self.optimizer.step()\n        self._zero_grad()\n        \n        #Reconversion\n        reconverted_outputs,inputs,labels,_ = self.encoder_decoder(next_style_inputs,inputs,1-labels,labels,lens)\n        reconverted_outputs = self.dropout(reconverted_outputs)\n        \n        #Classifier 3\n        reconverted_classifier_outputs = self.classifier(reconverted_outputs.unsqueeze(1))\n        \n        #Backprop\n        reconverted_scores = pack_padded_sequence(reconverted_outputs, (lens-1).squeeze(1).to(torch.device('cpu')), batch_first=True)\n        loss4 = self.criterion(reconverted_scores.data,trgs.data)\n        loss4.backward(retain_graph=True)\n        loss5 = self.criterion(reconverted_classifier_outputs,torch.tensor(labels,dtype=torch.long).squeeze(1))\n        loss5.backward()\n        clip_gradient(self.optimizer,grad_clip)\n        self.optimizer.step()\n        self._zero_grad()\n\n        return autoencoder_outputs, autoencoder_classifier_outputs,style_converted_classifier_outputs, reconverted_outputs,reconverted_classifier_outputs,sort_ind\n    \n    def update_model(self,autoencoder_outputs, autoencoder_classifier_outputs,style_converted_classifier_outputs, reconverted_outputs,reconverted_classifier_outputs,targets,labels,lens):\n        trgs = targets[:,1:]\n        trgs = pack_padded_sequence(trgs, (lens-1).to(torch.device('cpu')), batch_first=True)\n        \n        autoencoder_scores = pack_padded_sequence(autoencoder_outputs, (lens-1).to(torch.device('cpu')), batch_first=True)\n        reconverted_scores = pack_padded_sequence(reconverted_outputs, (lens-1).to(torch.device('cpu')), batch_first=True)\n        \n        loss1 = self.criterion(autoencoder_scores.data,trgs.data)\n        loss2 = self.criterion(autoencoder_classifier_outputs,labels)\n        loss3 = self.criterion(style_converted_classifier_outputs,1-labels)\n        loss4 = self.criterion(reconverted_scores.data,trgs.data)\n        loss5 = self.criterion(reconverted_classifier_outputs,labels)\n        loss1.backward(retain_graph=True)\n        loss2.backward(retain_graph=True)\n        loss3.backward(retain_graph=True)\n        loss4.backward(retain_graph=True)\n        loss5.backward()\n        clip_gradient(self.optimizer, grad_clip)\n        self.optimizer.step()\n    \n    def compute_loss(self,autoencoder_outputs, autoencoder_classifier_outputs,style_converted_classifier_outputs, reconverted_outputs,reconverted_classifier_outputs,targets,labels,lens):\n        trgs = targets[:,1:]\n        trgs = pack_padded_sequence(trgs, (lens-1).to(torch.device('cpu')), batch_first=True)\n        \n        autoencoder_scores = pack_padded_sequence(autoencoder_outputs, (lens-1).to(torch.device('cpu')), batch_first=True)\n        reconverted_scores = pack_padded_sequence(reconverted_outputs, (lens-1).to(torch.device('cpu')), batch_first=True)\n\n        loss1 = self.criterion(autoencoder_scores.data,trgs.data).item()\n        loss2 = self.criterion(autoencoder_classifier_outputs,labels).item()\n        loss3 = self.criterion(style_converted_classifier_outputs,1-labels).item()\n        loss4 = self.criterion(reconverted_scores.data,trgs.data).item()\n        loss5 = self.criterion(reconverted_classifier_outputs,labels).item()\n        print(loss1,loss2,loss3,loss4,loss5)\n        return loss1+loss2+loss3+loss4+loss5\n        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clip_gradient(optimizer, grad_clip):\n    \"\"\"\n    Clips gradients computed during backpropagation to avoid explosion of gradients.\n    :param optimizer: optimizer with the gradients to be clipped\n    :param grad_clip: clip value\n    \"\"\"\n    for group in optimizer.param_groups:\n        for param in group['params']:\n            if param.grad is not None:\n                param.grad.data.clamp_(-grad_clip, grad_clip)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embedding_layer = nn.Embedding(len(vocab),100)\nencoder = Encoder(100,200,embedding_layer)\none_step_decoder = BahdanauDecoder(100,200,len(vocab),embedding_layer)\ndecoder = Decoder(one_step_decoder,device)\nmodel = Model(encoder,decoder,embedding_layer,100,[1,2,3,4],2,5,30,vocab,device)\nmodel.to(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model.load_state_dict(torch.load('../input/abusive-language/model.pt'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_content_preservation_score(actual_word_lists, generated_word_lists, embedding_model):\n    #sentiment_words = lexicon_helper.get_sentiment_words()\n    cosine_distances = list()\n    skip_count = 0\n    for word_list_1, word_list_2 in zip(actual_word_lists, generated_word_lists):\n        cosine_similarity = 0\n        words_1 = set(word_list_1)\n        words_2 = set(word_list_2)\n\n        # words_1 -= sentiment_words\n        # words_2 -= sentiment_words\n        try:\n            cosine_similarity = 1 - cosine(\n                get_sentence_embedding(words_1, embedding_model),\n                get_sentence_embedding(words_2, embedding_model))\n            cosine_distances.append(cosine_similarity)\n        except ValueError:\n            skip_count += 1\n            logger.debug(\"Skipped lines: {} :-: {}\".format(word_list_1, word_list_2))\n\n    logger.debug(\"{} lines skipped due to errors\".format(skip_count))\n    mean_cosine_distance = statistics.mean(cosine_distances) if cosine_distances else 0\n\n    # del sentiment_words\n\n    return mean_cosine_distance","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class AverageMeter(object):\n    \"\"\"\n    Keeps track of most recent, average, sum, and count of a metric.\n    \"\"\"\n\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def accuracy(scores, targets, k,two_d=True):\n    \"\"\"\n    Computes top-k accuracy, from predicted and true labels.\n    :param scores: scores from the model\n    :param targets: true labels\n    :param k: k in top-k accuracy\n    :return: top-k accuracy\n    \"\"\"\n\n    batch_size = targets.size(0)\n    if two_d==True:\n        batch_size*=targets.size(1)\n    _, ind = scores.topk(k, -1,True, True)\n    ind = ind.squeeze(-1)\n    correct = ind.eq(targets)\n    correct_total = correct.view(-1).float().sum()  # 0D tensor\n    return correct_total.item() * (100.0 / batch_size)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\nfrom torch.nn.utils.rnn import pack_padded_sequence\n\n\ndef validate(val_loader, model,print_freq=100):\n    \"\"\"\n    Performs one epoch's validation.\n    :param val_loader: DataLoader for validation data.\n    :param encoder: encoder model\n    :param decoder: decoder model\n    :param criterion: loss layer\n    :return: BLEU-4 score\n    \"\"\"\n    model.eval()  # eval mode (no dropout or batchnorm)\n\n    batch_time = AverageMeter()\n    losses = AverageMeter()\n    top1accs = AverageMeter()\n    classifier_accs = AverageMeter()\n\n    start = time.time()\n\n    with torch.no_grad():\n        # Batches\n        for i, (caps, label,caplens) in enumerate(val_loader):\n\n            # Move to device, if available\n            caps = caps.to(device)\n            label = label.to(device)\n            caplens = caplens.to(device)\n\n            # Forward prop.\n            a,ac,sc,r,rc,sort_ind = model(caps,torch.tensor(label,dtype=torch.float),caplens)\n            caps_sorted = caps[sort_ind]\n            label_sorted = label[sort_ind]\n            caplens_sorted = caplens[sort_ind]\n            \n            # Since we decoded starting with <start>, the targets are all words after <start>, up to <end>\n\n            # Calculate loss\n            loss = model.compute_loss(a,ac,sc,r,rc,caps_sorted,label_sorted.squeeze(1),caplens_sorted.squeeze(1))\n            torch.cuda.empty_cache()\n\n#             # Add doubly stochastic attention regularization\n\n            # Keep track of metrics\n            losses.update(loss, sum(caplens_sorted.squeeze(1)-1))\n            top1 = (accuracy(a,caps_sorted[:,1:],1) + accuracy(r,caps_sorted[:,1:],1))/2.0\n            top1accs.update(top1,sum(caplens_sorted.squeeze(1)-1))\n            class_ac = (accuracy(ac,label_sorted.squeeze(-1),1) +accuracy(sc,(1-label_sorted).squeeze(-1),1) + accuracy(rc,label_sorted.squeeze(-1),1))/3.0\n            classifier_accs.update(class_ac,caps.shape[0])\n            batch_time.update(time.time() - start)\n\n            start = time.time()\n\n            if i % print_freq == 0:\n                print('Validation: [{0}/{1}]\\t'\n                      'Batch Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n                      'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n                      'Classifier Accuract {classifier.val:.3f} ({classifier.avg:.3f})\\t'.format(i, len(val_loader), batch_time=batch_time,loss=losses, classifier=classifier_accs))\n            \n                print('Top-1 Accuracy {top1.val:.3f} ({top1.avg:.3f})'.format(top1=top1accs))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print_freq=100\n\nbatch_time = AverageMeter()  # forward prop. + back prop. time\ndata_time = AverageMeter()  # data loading time\nlosses = AverageMeter()  # loss (per word decoded)\ntop1accs = AverageMeter()\nclassifier_accs=AverageMeter()\ngrad_clip = 5.\n\nepochs_since_improvement = 0\n\n# adjust_learning_rate(decoder_optimizer,0.8)\n\nstart = time.time()\nfor epoch in range(1):\n#     if epochs_since_improvement > 0 and epochs_since_improvement % 8 == 0:\n#             adjust_learning_rate(decoder_optimizer, 0.8)\n    model.train()\n    for i, (caps, label,caplens) in enumerate(train_dataloader):\n        \n        model._zero_grad()\n\n        # Move to device, if available\n        caps = caps.to(device)\n        label = label.to(device)\n        caplens = caplens.to(device)\n\n        # Forward prop.\n        a,ac,sc,r,rc,sort_ind = model(caps,torch.tensor(label,dtype=torch.float),caplens)\n        caps_sorted = caps[sort_ind]\n        label_sorted = label[sort_ind]\n        caplens_sorted = caplens[sort_ind]\n        \n        \n        # Calculate loss\n        loss = model.compute_loss(a,ac,sc,r,rc,caps_sorted,label_sorted.squeeze(1),caplens_sorted.squeeze(1))\n#         model.update_model(a,ac,sc,r,rc,caps_sorted,label_sorted.squeeze(1),caplens_sorted.squeeze(1))\n#         print(torch.cuda.memory_summary(device=None, abbreviated=False))\n        torch.cuda.empty_cache()\n            \n\n        # Keep track of metrics\n        losses.update(loss, sum(caplens_sorted.squeeze(1)-1))\n        top1 = (accuracy(a,caps_sorted[:,1:],1) + accuracy(r,caps_sorted[:,1:],1))/2.0\n        top1accs.update(top1,sum(caplens_sorted.squeeze(1)-1))\n        class_ac = (accuracy(ac,label_sorted.squeeze(-1),1,False) +accuracy(sc,(1-label_sorted).squeeze(-1),1,False) + accuracy(rc,label_sorted.squeeze(-1),1,False))/3.0\n        classifier_accs.update(class_ac,caps.shape[0])\n        batch_time.update(time.time() - start)\n\n        start = time.time()\n\n        if i % print_freq == 0:\n            print('Epoch: [{0}][{1}/{2}]\\t'\n                  'Batch Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n                  'Data Load Time {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'.format(epoch, i, len(train_dataloader),\n                                                                          batch_time=batch_time,\n                                                                          data_time=data_time, loss=losses))\n            print('Top-1 Accuracy {top1.val:.3f} ({top1.avg:.3f})'.format(top1=top1accs))\n            print('Classification Accuracy {classifier.val:.3f} ({classifier.avg:.3f})'.format(classifier=classifier_accs))\n    validate(validation_dataloader,model,print_freq)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# new_loader = []\n# for i,(caps,label,caplens) in enumerate(train_dataloader):\n#     ind = caplens.argmax(dim=0)\n#     new_loader.append((caps,label,caplens))\n#     if i%4==0:\n#         break\n# embedding_layer = nn.Embedding(len(vocab),100)\n# encoder = Encoder(100,200,embedding_layer)\n# one_step_decoder = BahdanauDecoder(100,200,len(vocab),embedding_layer)\n# decoder = Decoder(one_step_decoder,device)\n# model = Model(encoder,decoder,embedding_layer,100,[1,2,3,4],2,5,30,vocab,device)\n# model.to(device)\n# # for param_group in model.optimizer.param_groups:\n# #         param_group['lr'] = 0.0005\n# # print(model.optimizer)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(new_loader[0][0].shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def adjust_learning_rate(optimizer, shrink_factor):\n    \"\"\"\n    Shrinks learning rate by a specified factor.\n    :param optimizer: optimizer whose learning rate must be shrunk.\n    :param shrink_factor: factor in interval (0, 1) to multiply learning rate with.\n    \"\"\"\n\n    print(\"\\nDECAYING learning rate.\")\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = param_group['lr'] * shrink_factor\n    print(\"The new learning rate is %f\\n\" % (optimizer.param_groups[0]['lr'],))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import time\n\n# print_freq=100\n\n# batch_time = AverageMeter()  # forward prop. + back prop. time\n# data_time = AverageMeter()  # data loading time\n# losses = AverageMeter()  # loss (per word decoded)\n# top1accs = AverageMeter()\n# classifier_accs=AverageMeter()\n# grad_clip = 5.\n\n# best_loss=200\n\n# epochs_since_improvement = 0\n\n# # adjust_learning_rate(decoder_optimizer,0.8)\n\n# start = time.time()\n# for epoch in range(200):\n#     if epochs_since_improvement > 0 and epochs_since_improvement % 20 == 0:\n#             adjust_learning_rate(model.optimizer, 0.8)\n#     model.train()\n#     for i, (caps, label,caplens) in enumerate(new_loader):\n        \n#         model._zero_grad()\n\n#         # Move to device, if available\n#         caps = caps.to(device)\n#         label = label.to(device)\n#         caplens = caplens.to(device)\n\n#         # Forward prop.\n#         a,ac,sc,r,rc,sort_ind = model(caps,torch.tensor(label,dtype=torch.float),caplens)\n#         caps_sorted = caps[sort_ind]\n#         label_sorted = label[sort_ind]\n#         caplens_sorted = caplens[sort_ind]\n        \n        \n#         # Calculate loss\n#         loss = model.compute_loss(a,ac,sc,r,rc,caps_sorted,label_sorted.squeeze(1),caplens_sorted.squeeze(1))\n# #         model.update_model(a,ac,sc,r,rc,caps_sorted,label_sorted.squeeze(1),caplens_sorted.squeeze(1))\n# #         print(torch.cuda.memory_summary(device=None, abbreviated=False))\n#         torch.cuda.empty_cache()\n            \n\n#         # Keep track of metrics\n#         losses.update(loss, sum(caplens_sorted.squeeze(1)-1))\n# #         top1 = (accuracy(a,caps_sorted[:,1:],1) + accuracy(r,caps_sorted[:,1:],1))/2.0\n#         top1accs.update(accuracy(a,caps_sorted[:,1:],1),sum(caplens_sorted.squeeze(1)-1))\n# #         class_ac = (accuracy(ac,label_sorted.squeeze(-1),1,False) +accuracy(sc,(1-label_sorted).squeeze(-1),1,False) + accuracy(rc,label_sorted.squeeze(-1),1,False))/3.0\n# #         classifier_accs.update(accuracy(ac,(1-label_sorted).squeeze(-1),1,False),caps.shape[0])\n#         batch_time.update(time.time() - start)\n\n#         start = time.time()\n\n#         if i % print_freq == 0:\n#             print('Epoch: [{0}][{1}/{2}]\\t'\n#                   'Batch Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n#                   'Data Load Time {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n#                   'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'.format(epoch, i, len(new_loader),\n#                                                                           batch_time=batch_time,\n#                                                                           data_time=data_time, loss=losses))\n#             print('Top-1 Accuracy {top1.val:.3f} ({top1.avg:.3f})'.format(top1=top1accs))\n#             print('Classification Accuracy {classifier.val:.3f} ({classifier.avg:.3f})'.format(classifier=classifier_accs))\n# #     validate(validation_dataloader,model,print_freq)\n#         if loss<best_loss:\n#             best_loss=loss\n#             epochs_since_improvement=0\n#         else:\n#             epochs_since_improvement+=1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BeamSearchDecoder1(object):\n\n    def __init__(self, styleTransfer):\n        self.model = styleTransfer\n        self.max_length = self.model.max_len\n        self.width = self.model.beam_width\n\n    def _decode(self, tokens, encoder_outputs, h):\n        \"\"\"\n        Args:\n            tokens --\n            h --\n        Outputs:\n            logProbs --\n            indices --\n            h --\n        \"\"\"\n        currTokens = tokens.unsqueeze(1)\n        currh = h\n        # generate next h state and logit\n        # generator needs input (seq_len, batch_size, input_size)\n        vocabProbs, h, _ = self.model.decoder.one_step_decoder(currTokens, currh, encoder_outputs)\n        # beam search trick to prevent probs vanishing\n        logProbs = torch.log(vocabProbs)\n        # take the beam_with most probable words\n        logProbs, indices = torch.topk(logProbs, self.width, dim=-1)\n        return vocabProbs,logProbs, indices, h\n\n    def _beamDecode(self, encoder_outputs,h0):\n        \"\"\"\n        Returning the ids of the beam_width most probable sentences' words.\n        Args:\n            h0 -- the first hidden state of dim = dim_y + dim_z\n        \"\"\"\n        batch_size = h0.shape[1]\n        go = torch.tensor([self.model.vocab['<start>']] * batch_size,dtype=torch.long).to(self.model.device)\n        init_state = BeamState(\n            go,\n            h0,\n            [[] for _ in range(batch_size)],\n            [[self.model.vocab['<start>']] for i in range(batch_size)],\n            [0]*batch_size)\n        beam = [init_state]\n        for _ in range(self.max_length):\n            storeBeamLayer = [[] for _ in range(batch_size)]\n            for state in beam:\n                vocabProbs, logProbs, indices, h = self._decode(state.word,encoder_outputs, state.h)\n                for b in range(batch_size):\n                    for w in range(self.width):\n                        word = int(indices[b, w])\n                        storeBeamLayer[b].append(\n                            BeamState(word,\n                                      h[:, b, :],\n                                      state.outputs[b] + [vocabProbs[b]],\n                                      state.sentence[b] + [indices[b, w]],\n                                      state.nll[b] - logProbs[b, w]))\n\n            beam = [init_state for _ in range(self.width)]\n            for b in range(batch_size):\n                # sort beam states by their probability (cumulated nll)\n                # TODO check if performance increase by dividing nll\n                # by number of words\n                sortedBeamLayer = sorted(storeBeamLayer[b], key=lambda k: k.nll)\n                for w in range(self.width):\n                    beam[w].word[b] = sortedBeamLayer[w].word\n                    beam[w].h[:, b, :] = sortedBeamLayer[w].h\n                    beam[w].outputs[b] = sortedBeamLayer[w].outputs\n                    beam[w].sentence[b] = sortedBeamLayer[w].sentence\n                    beam[w].nll[b] = sortedBeamLayer[w].nll\n\n        # Returning the ids of the beam_width most probable sentences' words.\n        sentences = torch.tensor(beam[0].sentence,dtype=torch.long)\n        word_sentences = [\n            [self.model.vocab.itos[i] for i in sent]\n            for sent in sentences]\n        # TODO strip the EOS\n        word_sentences = list(map(lambda x: \" \".join(x), word_sentences))\n        \n        #Outputs of decoder at every time step\n        outputs = beam[0].outputs\n        for i in range(len(outputs)):\n            outputs[i] = torch.tensor([t.to(torch.device('cpu')).detach().numpy() for t in outputs[i]],dtype = torch.float32)\n        outputs = torch.tensor([t.numpy() for t in outputs],dtype=torch.float32)\n\n        return outputs.to(device),sentences,word_sentences,beam\n\n    def rewriteBatch(self, encoder_outputs,labels,word_id=True):\n        h0 = self.model.decoder.decoderLabelsTransform(labels)\n        h0 = h0.unsqueeze(0)\n        outputs = self._beamDecode(encoder_outputs,h0)\n        if word_id:\n            return outputs[0],torch.tensor(outputs[1],dtype=torch.long).to(device)\n        else:\n            return outputs[2],outputs[3]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sentence = new_loader[0][0]\nlabels = torch.tensor(new_loader[0][1],dtype=torch.float)\nprint([vocab.itos[x] for x in sentence[0]])\n# index = 0\n# sentence = tokenizer(data.iloc[index].tweet)\n# print(sentence,data.iloc[index].target)\n# sentence = [vocab[x] for x in sentence]\n# sentence = torch.tensor(sentence,dtype=torch.long).to(device)\n# sentence = sentence.unsqueeze(0)\n# labels = [data.iloc[index].target]\n# labels = torch.tensor(labels,dtype=torch.float).unsqueeze(1).to(device)\n# print(sentence.shape,labels.shape)\nencoder_outputs,_ = encoder(sentence,labels)\nbeam = BeamSearchDecoder1(model)\noutputs,outputs1 = beam.rewriteBatch(encoder_outputs,labels,False)\nprint(outputs[0])\ntorch.cuda.empty_cache()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i,(caps,labels,caplens) in enumerate(new_loader):\n    sentence = caps\n    labels = labels\n    lens=caplens","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(new_loader[0][0].shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output,_,_,_  = model.encoder_decoder(sentence,None,torch.tensor(labels,dtype=torch.float),torch.tensor(labels,dtype=torch.float),None)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(output.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ans,_ = output.topk(k=5,dim=2)\nprint()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fin_output = output.argmax(dim=2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print([vocab.itos[x] for x in fin_output[0]])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(model.state_dict(), 'model.pt')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model.load_state_dict(torch.load('model.pt'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# vocab['quin']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}